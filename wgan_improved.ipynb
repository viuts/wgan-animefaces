{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPupaOUSwnP7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jD1xUi0HwnP-"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import time\n",
    "import functools\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import pdb\n",
    "\n",
    "from models.wgan import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.autograd import grad\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUjCuFx8wnQB"
   },
   "outputs": [],
   "source": [
    "# lsun lmdb data set can be download via https://github.com/fyu/lsun\n",
    "# 64x64 ImageNet at http://image-net.org/small/download.php\n",
    "DATA_DIR = '/data/animefaces' # Replace your image data path here\n",
    "VAL_DIR = '/data/animefaces'\n",
    "IMAGE_DATA_SET = 'animefaces' \n",
    "# change this to something else, e.g. 'imagenets' or 'raw' if your data is just a folder of raw images. \n",
    "# Example: \n",
    "# IMAGE_DATA_SET = 'raw'\n",
    "# If you use lmdb, you'll need to write the loader by yourself. Please check load_data function\n",
    "\n",
    "TRAINING_CLASS = [] # IGNORE this if you are NOT training on lsun, or if you want to train on other classes of lsun, then change it accordingly\n",
    "VAL_CLASS = [] # IGNORE this if you are NOT training on lsun, or if you want to train on other classes of lsun, then change it accordingly\n",
    "\n",
    "if len(DATA_DIR) == 0:\n",
    "    raise Exception('Please specify path to data directory in gan_64x64.py!')\n",
    "\n",
    "RESTORE_MODE = True # if True, it will load saved model from OUT_PATH and continue to train\n",
    "START_ITER = 0 # starting iteration \n",
    "OUTPUT_PATH = '/content/drive/My Drive/data/models/animefaces/' # output path where result (.e.g drawing images, cost, chart) will be stored\n",
    "# MODE = 'wgan-gp'\n",
    "DIM = 64 # Model dimensionality\n",
    "CRITIC_ITERS = 5 # How many iterations to train the critic for\n",
    "GENER_ITERS = 1\n",
    "N_GPUS = 1 # Number of GPUs\n",
    "BATCH_SIZE = 64# Batch size. Must be a multiple of N_GPUS\n",
    "END_ITER = 100000 # How many iterations to train for\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter\n",
    "OUTPUT_DIM = 64*64*3 # Number of pixels in each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tl4aA7qswnQD"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, MyConvo2d): \n",
    "        if m.conv.weight is not None:\n",
    "            if m.he_init:\n",
    "                init.kaiming_uniform_(m.conv.weight)\n",
    "            else:\n",
    "                init.xavier_uniform_(m.conv.weight)\n",
    "        if m.conv.bias is not None:\n",
    "            init.constant_(m.conv.bias, 0.0)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        if m.weight is not None:\n",
    "            init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.0)\n",
    "\n",
    "def load_data(path_to_folder, classes):\n",
    "    data_transform = transforms.Compose([\n",
    "                 transforms.Resize(64),\n",
    "                 transforms.CenterCrop(64),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])\n",
    "                ])\n",
    "    if IMAGE_DATA_SET == 'lsun':\n",
    "        dataset =  datasets.LSUN(path_to_folder, classes=classes, transform=data_transform)\n",
    "    else:\n",
    "        dataset = datasets.ImageFolder(root=path_to_folder,transform=data_transform)\n",
    "    dataset_loader = torch.utils.data.DataLoader(dataset,batch_size=BATCH_SIZE, shuffle=True, num_workers=5, drop_last=True, pin_memory=True)\n",
    "    return dataset_loader\n",
    "\n",
    "def training_data_loader():\n",
    "    return load_data(DATA_DIR, TRAINING_CLASS) \n",
    "\n",
    "def val_data_loader():\n",
    "    return load_data(VAL_DIR, VAL_CLASS) \n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    alpha = torch.rand(BATCH_SIZE, 1)\n",
    "    alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement()/BATCH_SIZE)).contiguous()\n",
    "    alpha = alpha.view(BATCH_SIZE, 3, DIM, DIM)\n",
    "    alpha = alpha.to(device)\n",
    "    \n",
    "    fake_data = fake_data.view(BATCH_SIZE, 3, DIM, DIM)\n",
    "    interpolates = alpha * real_data.detach() + ((1 - alpha) * fake_data.detach())\n",
    "\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates.requires_grad_(True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)                              \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty\n",
    "\n",
    "def generate_image(netG, noise=None):\n",
    "    if noise is None:\n",
    "        noise = gen_rand_noise()\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \tnoisev = noise \n",
    "    samples = netG(noisev)\n",
    "    samples = samples.view(BATCH_SIZE, 3, 64, 64)\n",
    "    samples = samples * 0.5 + 0.5\n",
    "    return samples\n",
    "\n",
    "def gen_rand_noise():\n",
    "    noise = torch.randn(BATCH_SIZE, 128)\n",
    "    noise = noise.to(device)\n",
    "\n",
    "    return noise\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "fixed_noise = gen_rand_noise() \n",
    "\n",
    "if RESTORE_MODE:\n",
    "    aG = torch.load(OUTPUT_PATH + \"generator.pt\")\n",
    "    aD = torch.load(OUTPUT_PATH + \"discriminator.pt\")\n",
    "else:\n",
    "    aG = GoodGenerator(64,64*64*3)\n",
    "    aD = GoodDiscriminator(64)\n",
    "    \n",
    "    aG.apply(weights_init)\n",
    "    aD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXVB46GRASPY"
   },
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "optimizer_g = torch.optim.Adam(aG.parameters(), lr=LR, betas=(0,0.9))\n",
    "optimizer_d = torch.optim.Adam(aD.parameters(), lr=LR, betas=(0,0.9))\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "aG = aG.to(device)\n",
    "aD = aD.to(device)\n",
    "one = one.to(device)\n",
    "mone = mone.to(device)\n",
    "writer = SummaryWriter(OUTPUT_PATH + 'runs/exp-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukSY_DT0skQ0"
   },
   "outputs": [],
   "source": [
    "fixed_noise = gen_rand_noise() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FlRiI4vwEu8r"
   },
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "ncritic = 5\n",
    "current_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zT5bcULlE39T"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    global current_iter\n",
    "    dataloader = training_data_loader() \n",
    "    dataiter = iter(dataloader)\n",
    "    with tqdm(initial=current_iter,total=END_ITER) as pbar:\n",
    "      for iteration in range(current_iter, END_ITER):\n",
    "          #---------------------TRAIN G------------------------\n",
    "          for p in aD.parameters():\n",
    "              p.requires_grad_(False)  # freeze D\n",
    "\n",
    "          gen_cost = None\n",
    "          for i in range(GENER_ITERS):\n",
    "              aG.zero_grad()\n",
    "              noise = gen_rand_noise()\n",
    "              noise.requires_grad_(True)\n",
    "              fake_data = aG(noise)\n",
    "              gen_cost = aD(fake_data)\n",
    "              gen_cost = gen_cost.mean()\n",
    "              gen_cost.backward(mone)\n",
    "              gen_cost = -gen_cost\n",
    "\n",
    "          optimizer_g.step()\n",
    "          #---------------------TRAIN D------------------------\n",
    "          for p in aD.parameters():  # reset requires_grad\n",
    "              p.requires_grad_(True)  # they are set to False below in training G\n",
    "          for i in range(CRITIC_ITERS):\n",
    "\n",
    "              aD.zero_grad()\n",
    "\n",
    "              # gen fake data and load real data\n",
    "              noise = gen_rand_noise()\n",
    "              with torch.no_grad():\n",
    "                  noisev = noise  # totally freeze G, training D\n",
    "              fake_data = aG(noisev).detach()\n",
    "              batch = next(dataiter, None)\n",
    "              if batch is None:\n",
    "                  dataiter = iter(dataloader)\n",
    "                  batch = dataiter.next()\n",
    "              batch = batch[0] #batch[1] contains labels\n",
    "              real_data = batch.to(device) #TODO: modify load_data for each loading\n",
    "\n",
    "              # train with real data\n",
    "              disc_real = aD(real_data)\n",
    "              disc_real = disc_real.mean()\n",
    "\n",
    "              # train with fake data\n",
    "              disc_fake = aD(fake_data)\n",
    "              disc_fake = disc_fake.mean()\n",
    "\n",
    "              # train with interpolates data\n",
    "              gradient_penalty = calc_gradient_penalty(aD, real_data, fake_data)\n",
    "\n",
    "              # final disc cost\n",
    "              disc_cost = disc_fake - disc_real + gradient_penalty\n",
    "              disc_cost.backward()\n",
    "              w_dist = disc_fake  - disc_real\n",
    "              optimizer_d.step()\n",
    "              #------------------VISUALIZATION----------\n",
    "              if i == CRITIC_ITERS-1:\n",
    "                  writer.add_scalar('runs/disc_cost', disc_cost, iteration)\n",
    "                  writer.add_scalar('data/disc_fake', disc_fake, iteration)\n",
    "                  writer.add_scalar('data/disc_real', disc_real, iteration)\n",
    "                  writer.add_scalar('runs/gradient_pen', gradient_penalty, iteration)\n",
    "                  #writer.add_scalar('data/d_conv_weight_mean', [i for i in aD.children()][0].conv.weight.data.clone().mean(), iteration)\n",
    "                  #writer.add_scalar('data/d_linear_weight_mean', [i for i in aD.children()][-1].weight.data.clone().mean(), iteration)\n",
    "                  #writer.add_scalar('data/fake_data_mean', fake_data.mean(), iteration)\n",
    "                  #writer.add_scalar('data/real_data_mean', real_data.mean(), iteration)\n",
    "                  #if iteration %200==99:\n",
    "                  #    paramsD = aD.named_parameters()\n",
    "                  #    for name, pD in paramsD:\n",
    "                  #      writer.add_histogram(\"D.\" + name, pD.clone().data.cpu().numpy(), iteration)\n",
    "\n",
    "          #---------------VISUALIZATION---------------------\n",
    "          writer.add_scalar('runs/gen_cost', gen_cost, iteration)\n",
    "          writer.add_scalar('runs/wasserstein_distance', w_dist, iteration)\n",
    "\n",
    "          if iteration % 200 == 199:\n",
    "              gen_images = generate_image(aG, fixed_noise)\n",
    "              torchvision.utils.save_image(gen_images, OUTPUT_PATH + 'images/samples_{}.png'.format(iteration), nrow=8, padding=2)\n",
    "              grid_images = torchvision.utils.make_grid(gen_images, nrow=8, padding=2)\n",
    "              writer.add_image('runs/images', grid_images, iteration)\n",
    "    #----------------------Save model----------------------\n",
    "              torch.save(aG, OUTPUT_PATH + \"generator.pt\")\n",
    "              torch.save(aD, OUTPUT_PATH + \"discriminator.pt\")\n",
    "              torch.save(aG.state_dict(), OUTPUT_PATH + \"generator_state.pt\")\n",
    "              torch.save(aD.state_dict(), OUTPUT_PATH + \"discriminator_state.pt\")\n",
    "          pbar.update()\n",
    "          current_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d7RQU8M-wnQb",
    "outputId": "43c1d514-d24c-4f17-c993-6a0ad73de9f1"
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4B1PSOghEliS"
   },
   "outputs": [],
   "source": [
    "def plot_batch(title, image_list):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(1,1,1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.imshow(image_list)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQmEUh81Dh-x"
   },
   "outputs": [],
   "source": [
    "def compare_real_fake():\n",
    "    # Grab a batch of real images from the dataloader\n",
    "    real_batch = next(iter(training_data_loader()))\n",
    "    real_images = torchvision.utils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu()\n",
    "    fake_images = torchvision.utils.make_grid(generate_image(), nrow=8, padding=2)\n",
    "\n",
    "    # Plot the real images\n",
    "    plot_batch(\"Real Images\", real_images)\n",
    "\n",
    "    # Plot the fake images from the last epoch\n",
    "    plot_batch(\"Fake Images\", fake_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pleKzEktwnQi"
   },
   "source": [
    "**Real Images vs.Â Fake Images**\n",
    "\n",
    "Finally, lets take a look at some real images and fake images side by\n",
    "side.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhEmMJALwnQj"
   },
   "outputs": [],
   "source": [
    "compare_real_fake()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wgan_improved",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
